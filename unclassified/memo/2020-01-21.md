# 2020-01-21

<br>

## 미니 배치로 나누는 이유

* 데이터를 한 번에 연산할 수 없어서
* 확률적으로 더 빨리 수렴하고 싶어서



<br>

## 하이퍼 파라미터

* weight 초기값
* nonlinearity
* optimizers
* batch normalization
* dropout
* ensemble



<br>

## Xavier 초기화

* sigmoid 에서 사용?



<br>

## He 초기화

* relu에 사용



<br>

## sigmoid, softmax

* sigmoid의 출력 0~1을 총합 1 로 만들어주는 역할 softmax



<br>

## 모멘텀

* 진행중인 방향으로 관성을 더해 지역최소점에 빠지지 않도록 함
* 가변 모멘텀 : Adam(Adaptive Moment Estimation)?



<br>

## batch normalization, dropout

* batch : input, hidden
* dropout : input, hidden
  1. add Dense
  2. add BatchNormalization
  3. add Dropout
  4. add activation



<br>

## dropout, minibatch의 앙상블효과

* dropout 을통해 여러개의 신경망을 만드는 느낌
* dropout 을통해 신경망의 다형성을 높임
* 보통 0.5 이상?



<br>

## 선형함수의 한계

* 선형함수의 조합은 비선형이 될 수 없다.
* 선형함수의 조합은 비선형 문제를 풀 수 없다.



<br>

## cnn 과정 분리

* 컨볼루션, 맥스풀링은 학습된 것을 사용하기도 함, 시간이 많이 걸리기 때문
* 파라미터가 적은 mlp 층만 커스터마이즈해서 사용하기도 함
* mlp + 컨볼루션 상위층을 커스터마이즈해서 사용하기도 함



## flatten

* 컨볼루션+맥스풀링층 에서 mlp 층으로갈때 줌

* flatten 다음 첫 layer 뉴런수는 0.3배정도??